{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SkIn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1FaBkOzNVdd4c76vgctIGcNWtjY_ymJ49",
      "authorship_tag": "ABX9TyM/2plMzhnWSoSNbXN5NHvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagunkayastha/Cell_Seg_count/blob/master/SkIn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDnztixcgkEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "53487705-8514-494a-ed6d-3ae658aa65ca"
      },
      "source": [
        "!pip install efficientnet_pytorch\n",
        "!cp /content/drive/My\\ Drive/Datasets/skin_data.zip ./\n",
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir\n",
        "!unzip -q skin_data.zip -d data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-9lbghjjp\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-9lbghjjp\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: imgaug>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-cp36-none-any.whl size=67247 sha256=72d762820c551256037c3ddf57d058634ad6ac125f6931e8d405b090ae0f81c1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ub59rgvs/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
            "Successfully built albumentations\n",
            "Installing collected packages: albumentations\n",
            "  Found existing installation: albumentations 0.4.6\n",
            "    Uninstalling albumentations-0.4.6:\n",
            "      Successfully uninstalled albumentations-0.4.6\n",
            "Successfully installed albumentations-0.4.6\n",
            "replace data/sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFy6y19FroRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from torchvision import transforms as T\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torchvision import models\n",
        "import pdb\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from sklearn import model_selection\n",
        "import time\n",
        "import albumentations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pRr6F6bT1S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/data'\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "Batch_size = 32\n",
        "image_size = 216"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_1CDDdPVbTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "271d5751-4f2e-4921-b7d6-812421f5b65a"
      },
      "source": [
        "df = pd.read_csv(os.path.join(data_path,'train.csv'))\n",
        "df[\"kfold\"] = -1    \n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "y = df.target.values\n",
        "kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "\n",
        "df.to_csv(\"train_folds.csv\", index=False)\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>sex</th>\n",
              "      <th>age_approx</th>\n",
              "      <th>anatom_site_general_challenge</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>benign_malignant</th>\n",
              "      <th>target</th>\n",
              "      <th>tfrecord</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_3394796</td>\n",
              "      <td>IP_6767827</td>\n",
              "      <td>female</td>\n",
              "      <td>40.0</td>\n",
              "      <td>torso</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_9274041</td>\n",
              "      <td>IP_0612651</td>\n",
              "      <td>female</td>\n",
              "      <td>25.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6000</td>\n",
              "      <td>4000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_1241680</td>\n",
              "      <td>IP_0718832</td>\n",
              "      <td>female</td>\n",
              "      <td>55.0</td>\n",
              "      <td>torso</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1872</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_3455749</td>\n",
              "      <td>IP_0660290</td>\n",
              "      <td>male</td>\n",
              "      <td>60.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>6000</td>\n",
              "      <td>4000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_1672458</td>\n",
              "      <td>IP_9927968</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>head/neck</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     image_name  patient_id     sex  age_approx  ... tfrecord width height  kfold\n",
              "0  ISIC_3394796  IP_6767827  female        40.0  ...       13   640    480      0\n",
              "1  ISIC_9274041  IP_0612651  female        25.0  ...        5  6000   4000      0\n",
              "2  ISIC_1241680  IP_0718832  female        55.0  ...       10  1872   1053      0\n",
              "3  ISIC_3455749  IP_0660290    male        60.0  ...       10  6000   4000      0\n",
              "4  ISIC_1672458  IP_9927968    male        35.0  ...        4   640    480      0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IXH57KmWp8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d1c7bd1-c98a-4966-93ae-0a3c2f4d4193"
      },
      "source": [
        "fold=0\n",
        "df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "len(df_train),len(df_valid)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26500, 6626)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl3bQdDLWs2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScancerDataset(Dataset):\n",
        "    def __init__(self, df, data_folder, transforms=None):\n",
        "        self.df = df\n",
        "        self.data_folder = data_folder\n",
        "        self.transforms_object = transforms\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.df.loc[idx,'image_name']\n",
        "        img_path = os.path.join(self.data_folder,img_name+'.jpg')\n",
        "        image_data = Image.open(img_path)\n",
        "        \n",
        "        if self.transforms_object:\n",
        "            image_data = self.transforms_object(image_data)\n",
        "        if 'target' in self.df.columns.values:\n",
        "            y = self.df.loc[idx,'target']\n",
        "        else :\n",
        "            y = 1\n",
        "        return image_data ,torch.tensor([target],dtype=torch.float32)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Kybo2jWuRJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "312ec096-4d97-4799-ce80-cfb43fe150a4"
      },
      "source": [
        "effic = EfficientNet.from_pretrained('efficientnet-b1')\n",
        "def GlobalAveragePooling(x):\n",
        "    return x.mean(axis=-1).mean(axis=-1)\n",
        "\n",
        "class Scancer(nn.Module):\n",
        "    def __init__(self, efn):\n",
        "        super(Scancer, self).__init__()\n",
        "        # self.dense_output = nn.Linear(features, 1)\n",
        "        self.efn = efn\n",
        "        self.efn._fc = nn.Linear(in_features=1280,out_features=512, bias=True)\n",
        "        \n",
        "        self.model= nn.Sequential(self.efn,\n",
        "                                     nn.Linear(512,128),\n",
        "                                     nn.LeakyReLU(),\n",
        "                                     nn.Linear(128,16),\n",
        "                                     nn.LeakyReLU(),\n",
        "                                     nn.Linear(16,1))\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dKltZbdPm9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
        "    Arguments:\n",
        "        indices (list, optional): a list of indices\n",
        "        num_samples (int, optional): number of samples to draw\n",
        "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
        "\n",
        "        # if indices is not provided, \n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # define custom callback\n",
        "        self.callback_get_label = callback_get_label\n",
        "\n",
        "        # if num_samples is not provided, \n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "\n",
        "        # distribution of classes in the dataset \n",
        "        label_to_count = {}\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label in label_to_count:\n",
        "                label_to_count[label] += 1\n",
        "            else:\n",
        "                label_to_count[label] = 1\n",
        "\n",
        "        # weight for each sample\n",
        "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
        "                   for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, idx):  \n",
        "        return dataset.train_labels[idx].item()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in torch.multinomial(\n",
        "            self.weights, self.num_samples, replacement=True))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn6gFP_qPprS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlLRxPNDKy57",
        "colab_type": "text"
      },
      "source": [
        "## AUG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UJzmTAUKysN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/159476\n",
        "## Roman microscope aug\n",
        "class Microscope:\n",
        "    def __init__(self, p: float = 0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            circle = cv2.circle((np.ones(img.shape) * 255).astype(np.uint8),\n",
        "                        (img.shape[0]//2, img.shape[1]//2),\n",
        "                        random.randint(img.shape[0]//2 - 3, img.shape[0]//2 + 15),\n",
        "                        (0, 0, 0),\n",
        "                        -1)\n",
        "\n",
        "            mask = circle - 255\n",
        "            img = np.multiply(img, mask)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}(p={self.p})'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x6SKdX4Wzhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76157448-46dd-48d0-dc35-9941e78eb4f8"
      },
      "source": [
        "train_transform = T.Compose([\n",
        "    T.RandomResizedCrop(size=256,scale=(0.7,1)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomVerticalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = ScancerDataset(df_train,os.path.join(data_path,'train'),transforms=train_transform )\n",
        "valid_dataset = ScancerDataset(df_valid,os.path.join(data_path,'train'),transforms=train_transform )\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=Batch_size, shuffle=True, num_workers=4\n",
        "    )\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=Batch_size, shuffle=False, num_workers=4\n",
        "    )\n",
        "print(len)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<built-in function len>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UriOBY5HOpgF",
        "colab_type": "text"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw3grDcVQK3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://arxiv.org/abs/1708.02002\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol25Uwm3RBsD",
        "colab_type": "text"
      },
      "source": [
        "### TEST 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLfFH-FSWz9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "s_model= Scancer(effic)\n",
        "s_model.to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def bce(y_true, y_pred):\n",
        "    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "    y_true = y_true.squeeze()\n",
        "    y_pred = nn.Sigmoid()(y_pred).squeeze()\n",
        "    return (y_true == torch.round(y_pred)).float().sum()/len(y_true)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwzz2QI3W2MD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38c5032d-4f29-451b-8c16-330fe2de5652"
      },
      "source": [
        "train_batches = len(train_loader) - 1\n",
        "EPOCHS=10\n",
        "mb = master_bar(range(EPOCHS))\n",
        "mb.write(['epoch','train_loss'],table=True)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print((\"EPOCH %s\" + str(epoch+1) + \"%s\") )\n",
        "    \n",
        "    batch = 1\n",
        "    s_model.train()\n",
        "    trn_loss,val_loss = 0.0,0.0\n",
        "    for train_batch in  train_loader:\n",
        "        train_img, train_targ = train_batch\n",
        "        train_targ = train_targ.view(-1, 1)\n",
        "        train_img, train_targ = train_img.to(device), train_targ.to(device)\n",
        "        \n",
        "        if batch >= train_batches: break\n",
        "        train_preds = s_model.forward(train_img)\n",
        "        train_acc = acc(train_targ, train_preds)\n",
        "        train_loss = bce(train_targ, train_preds)\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        trn_loss +=train_loss\n",
        "        \n",
        "        if batch%100==0:\n",
        "          print(train_loss/Batch_size,train_acc)\n",
        "        batch = batch + 1\n",
        "        # print(batch)  \n",
        "    loss =trn_loss/ len(train_loader)  \n",
        "    \n",
        "    print(loss)\n",
        "        # batch = batch + 1\n",
        "        # accuracy = np.round(train_acc.item(), 3)\n",
        "        # print_metric(accuracy, batch, 0, start, end, metric=\"acc\", typ=\"Train\")\n",
        "        \n",
        "            \n",
        "    # network.eval()\n",
        "    # val_loss, val_acc, val_points = 0, 0, 0\n",
        "        \n",
        "    # with torch.no_grad():\n",
        "    #     for val_batch in tqdm(val_loader):\n",
        "    #         val_img, val_targ = val_batch\n",
        "    #         val_targ = val_targ.view(-1, 1)\n",
        "    #         val_img, val_targ = val_img.to(device), val_targ.to(device)\n",
        "\n",
        "    #         val_points += len(val_targ)\n",
        "    #         val_preds = s_model.forward(val_img)\n",
        "    #         val_acc += acc(val_targ, val_preds).item()*len(val_preds)\n",
        "    #         val_loss += bce(val_targ, val_preds).item()*len(val_preds)\n",
        "        \n",
        "    \n",
        "    # val_acc /= val_points\n",
        "    # val_loss /= val_points\n",
        "    # accuracy = np.round(val_acc, 3)\n",
        "    # # print_metric(accuracy, 0, epoch, start, end, metric=\"acc\", typ=\"Val\")\n",
        "    \n",
        "    print(\"\")\n",
        "\n",
        "print(\"ENDING TRAINING ...\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH %s1%s\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s2%s\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9531, device='cuda:0')\n",
            "tensor(0.0021, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s3%s\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0022, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s4%s\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0019, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s5%s\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0018, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s6%s\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9531, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0019, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s7%s\n",
            "tensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>) tensor(1., device='cuda:0')\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s8%s\n",
            "tensor(0.0018, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s9%s\n",
            "tensor(0.0018, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0022, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "EPOCH %s10%s\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9844, device='cuda:0')\n",
            "tensor(0.0021, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9688, device='cuda:0')\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "ENDING TRAINING ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0Vd3D8REfy",
        "colab_type": "text"
      },
      "source": [
        "### Test 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0LSSfXtg2QT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScancerDataset(Dataset):\n",
        "    def __init__(self, df, data_folder, transforms=None):\n",
        "        self.df = df\n",
        "        self.data_folder = data_folder\n",
        "        self.transform = transforms\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.df.loc[idx,'image_name']\n",
        "        img_path = os.path.join(self.data_folder,img_name+'.jpg')\n",
        "        \n",
        "        image = cv2.imread(img_path )\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            res = self.transform(image=image)\n",
        "            image = res['image'].astype(np.float32)\n",
        "        else:\n",
        "            image = image.astype(np.float32)\n",
        "\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        if 'target' in self.df.columns.values:\n",
        "            y = self.df.loc[idx,'target']\n",
        "        else :\n",
        "            y = 1\n",
        "        return torch.tensor(image).float(),torch.tensor([y],dtype=torch.float32)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y1Ss0WbhJpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "94db606a-0db7-49d9-dd53-86d588cfb57b"
      },
      "source": [
        "Batch_size=64\n",
        "train_transform = albumentations.Compose([\n",
        "        albumentations.Transpose(p=0.5),\n",
        "        albumentations.VerticalFlip(p=0.5),\n",
        "        albumentations.HorizontalFlip(p=0.5),\n",
        "        albumentations.RandomBrightness(limit=0.2, p=0.75),\n",
        "        albumentations.RandomContrast(limit=0.2, p=0.75),\n",
        "        albumentations.OneOf([\n",
        "            albumentations.MotionBlur(blur_limit=5),\n",
        "            albumentations.MedianBlur(blur_limit=5),\n",
        "            albumentations.GaussianBlur(blur_limit=5),\n",
        "            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n",
        "        ], p=0.7),\n",
        "\n",
        "        albumentations.OneOf([\n",
        "            albumentations.OpticalDistortion(distort_limit=1.0),\n",
        "            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n",
        "            albumentations.ElasticTransform(alpha=3),\n",
        "        ], p=0.7),\n",
        "\n",
        "        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n",
        "        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
        "        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
        "        albumentations.Resize(image_size, image_size),\n",
        "        albumentations.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),\n",
        "        albumentations.Normalize()\n",
        "    ])\n",
        "\n",
        "\n",
        "transforms_val = A.Compose([\n",
        "    A.Resize(image_size, image_size),\n",
        "    A.Normalize()\n",
        "])\n",
        "\n",
        "train_dataset = ScancerDataset(df_train,os.path.join(data_path,'train'),transforms=train_transform )\n",
        "valid_dataset = ScancerDataset(df_valid,os.path.join(data_path,'train'),transforms=train_transform )\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=Batch_size, shuffle=True, num_workers=4\n",
        "    )\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=Batch_size, shuffle=False, num_workers=4\n",
        "    )\n",
        "print(len(train_loader))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/transforms.py:2552: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
            "  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1RCRwF8YVCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "3c37635b-3b4f-49d0-97a1-5c27a8351d14"
      },
      "source": [
        "model= Scancer(effic)\n",
        "model.to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "      start_time = time.time()\n",
        "      correct = 0\n",
        "      running_loss = 0\n",
        "      total_predictions = 0.0\n",
        "      model.train()\n",
        "      count=0\n",
        "      for train_batch in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          train_img, train_targ = train_batch\n",
        "          \n",
        "          x,y = train_img.float().to(device), train_targ.to(device)\n",
        "          z = model(x)\n",
        "\n",
        "          \n",
        "          loss = criterion(z, y.reshape(-1,1).float())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          pred = torch.round(torch.sigmoid(z))  # round off sigmoid to obtain predictions\n",
        "          total_predictions += y.size(0)\n",
        "          correct  += (y.squeeze().cpu() ==pred.squeeze().cpu()).sum().item()\n",
        "          \n",
        "          running_loss += loss.item()\n",
        "          \n",
        "          if count%40==0:\n",
        "            print(count)\n",
        "          count+=1\n",
        "          #   test_loss = 0\n",
        "          #   accuracy = 0\n",
        "          #   model.eval()\n",
        "          #   with torch.no_grad():\n",
        "          #     for inputs, labels in valid_loader:\n",
        "          #       inputs, labels = inputs.to(device), labels.to(device)\n",
        "          #       logps = model.forward(inputs)\n",
        "          #       batch_loss = criterion(logps, labels.reshape(-1,1).float())\n",
        "          #       test_loss += batch_loss.item()\n",
        "\n",
        "          #       ps = torch.sigmoid(z)\n",
        "          #       top_p, top_class = ps.topk(1, dim=1)\n",
        "          #       equals = top_class == labels.view(*top_class.shape)\n",
        "          #       accuracy += torch.mean(equals.squeeze().type(torch.FloatTensor)).item()\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "          # count+=1\n",
        "          \n",
        "          \n",
        "\n",
        "           \n",
        "      acc = (correct/total_predictions)*100.0\n",
        "      running_loss /= len(train_loader)\n",
        "      end_time = time.time()\n",
        "      print('Training Loss: ', running_loss, 'Time: ',round(end_time - start_time,3), 's')\n",
        "      print('Training Accuracy: ', round(acc,3), '%')\n",
        "      # model.eval()  # switch model to the evaluation mode\n",
        "      # val_preds = torch.zeros((Batch_size, 1), dtype=torch.float32, device=device)\n",
        "      # with torch.no_grad():  # Do not calculate gradient since we are only predicting\n",
        "      #     # Predicting on validation set\n",
        "      #     for val_batch in enumerate(valid_loader):\n",
        "      #         x_val,y_val = train_img.to(device), train_targ.to(device)\n",
        "      # #         x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
        "      # #         y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
        "      #         z_val = model(x_val)\n",
        "      #         val_pred = torch.sigmoid(z_val)\n",
        "      # #         val_preds[j*x_val[0].shape[0]:j*x_val[0].shape[0] + x_val[0].shape[0]] = val_pred\n",
        "      #         print()\n",
        "      #     val_acc = accuracy_score(y_val.cpu(), torch.round(val_preds.cpu()))\n",
        "      #     val_roc = roc_auc_score(y_val.cpu(), val_preds.cpu())\n",
        "          \n",
        "      #     print('Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}'.format(\n",
        "      #     epoch + 1, \n",
        "      #     epoch_loss, \n",
        "      #     train_acc, \n",
        "      #     val_acc, \n",
        "      #     val_roc, \n",
        "      #     str(datetime.timedelta(seconds=time.time() - start_time))[:7]))\n",
        "          \n",
        "      #     scheduler.step(val_roc)\n",
        "      #     # During the first iteration (first epoch) best validation is set to None\n",
        "      #     if not best_val:\n",
        "      #         best_val = val_roc  # So any validation roc_auc we have is the best one for now\n",
        "      #         torch.save(model, model_path)  # Saving the model\n",
        "      #         continue\n",
        "              \n",
        "      #     if val_roc >= best_val:\n",
        "      #         best_val = val_roc\n",
        "      #         patience = es_patience  # Resetting patience since we have new best validation accuracy\n",
        "      #         torch.save(model, model_path)  # Saving current best model\n",
        "      #     else:\n",
        "      #         patience -= 1\n",
        "      #         if patience == 0:\n",
        "      #             print('Early stopping. Best Val roc_auc: {:.3f}'.format(best_val))\n",
        "      #             break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "40\n",
            "80\n",
            "120\n",
            "160\n",
            "200\n",
            "240\n",
            "280\n",
            "320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukeDiha3Qmt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0431cc12-6611-4491-b6de-06fe7c13519c"
      },
      "source": [
        "(y.cpu()==pred.squeeze().cpu()).sum()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4096)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEa3Ucdpy31e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "716f87ed-fd76-4f83-ba84-a599af634a44"
      },
      "source": [
        "y.squeeze().shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtaZggFW4sfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a358f4f5-d0db-479e-8657-33fb647db52e"
      },
      "source": [
        "pred.squeeze().shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEY7xVgc4vmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "094f141e-db2d-4680-d5c2-4046836ea6a3"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACBmXmHZPYfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0 1 0 0 0  0  == [0 1 1 0 0 ]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}